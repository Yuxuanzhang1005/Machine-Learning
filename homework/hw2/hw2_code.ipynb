{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  1., 10.],\n",
      "        [ 1.,  1.,  1.]])\n",
      "tensor([-1.,  1.,  1.])\n",
      "tensor([[0.1000],\n",
      "        [0.1000]])\n",
      "Loss: 0.526207; ||g||: 1.151119\n",
      "Loss: 0.169677; ||g||: 0.150690\n",
      "Loss: 0.148274; ||g||: 0.133631\n",
      "Loss: 0.131357; ||g||: 0.119766\n",
      "Loss: 0.117707; ||g||: 0.108330\n",
      "Loss: 0.106496; ||g||: 0.098769\n",
      "Loss: 0.097145; ||g||: 0.090677\n",
      "Loss: 0.089240; ||g||: 0.083753\n",
      "Loss: 0.082479; ||g||: 0.077769\n",
      "Loss: 0.076636; ||g||: 0.072552\n",
      "Loss: 0.071540; ||g||: 0.067967\n",
      "Loss: 0.067060; ||g||: 0.063909\n",
      "Loss: 0.063092; ||g||: 0.060294\n",
      "Loss: 0.059555; ||g||: 0.057055\n",
      "Loss: 0.056384; ||g||: 0.054136\n",
      "Loss: 0.053525; ||g||: 0.051495\n",
      "Loss: 0.050936; ||g||: 0.049093\n",
      "Loss: 0.048580; ||g||: 0.046900\n",
      "Loss: 0.046428; ||g||: 0.044890\n",
      "Loss: 0.044454; ||g||: 0.043042\n",
      "Loss: 0.042638; ||g||: 0.041337\n",
      "Loss: 0.040962; ||g||: 0.039760\n",
      "Loss: 0.039411; ||g||: 0.038296\n",
      "Loss: 0.037971; ||g||: 0.036934\n",
      "Loss: 0.036630; ||g||: 0.035664\n",
      "Loss: 0.035379; ||g||: 0.034477\n",
      "Loss: 0.034210; ||g||: 0.033365\n",
      "Loss: 0.033114; ||g||: 0.032322\n",
      "Loss: 0.032085; ||g||: 0.031341\n",
      "Loss: 0.031118; ||g||: 0.030417\n",
      "Loss: 0.030206; ||g||: 0.029545\n",
      "Loss: 0.029345; ||g||: 0.028721\n",
      "Loss: 0.028532; ||g||: 0.027941\n",
      "Loss: 0.027761; ||g||: 0.027201\n",
      "Loss: 0.027031; ||g||: 0.026500\n",
      "Loss: 0.026338; ||g||: 0.025833\n",
      "Loss: 0.025679; ||g||: 0.025198\n",
      "Loss: 0.025051; ||g||: 0.024594\n",
      "Loss: 0.024453; ||g||: 0.024017\n",
      "Loss: 0.023883; ||g||: 0.023467\n",
      "Loss: 0.023339; ||g||: 0.022941\n",
      "Loss: 0.022818; ||g||: 0.022438\n",
      "Loss: 0.022320; ||g||: 0.021956\n",
      "Loss: 0.021843; ||g||: 0.021494\n",
      "Loss: 0.021386; ||g||: 0.021051\n",
      "Loss: 0.020947; ||g||: 0.020626\n",
      "Loss: 0.020526; ||g||: 0.020218\n",
      "Loss: 0.020121; ||g||: 0.019825\n",
      "Loss: 0.019732; ||g||: 0.019447\n",
      "Loss: 0.019357; ||g||: 0.019083\n",
      "Loss: 0.018997; ||g||: 0.018732\n",
      "Loss: 0.018649; ||g||: 0.018394\n",
      "Loss: 0.018314; ||g||: 0.018067\n",
      "Loss: 0.017990; ||g||: 0.017752\n",
      "Loss: 0.017678; ||g||: 0.017448\n",
      "Loss: 0.017376; ||g||: 0.017154\n",
      "Loss: 0.017084; ||g||: 0.016869\n",
      "Loss: 0.016802; ||g||: 0.016594\n",
      "Loss: 0.016529; ||g||: 0.016328\n",
      "Loss: 0.016264; ||g||: 0.016069\n",
      "Loss: 0.016008; ||g||: 0.015819\n",
      "Loss: 0.015760; ||g||: 0.015577\n",
      "Loss: 0.015519; ||g||: 0.015341\n",
      "Loss: 0.015285; ||g||: 0.015113\n",
      "Loss: 0.015058; ||g||: 0.014891\n",
      "Loss: 0.014838; ||g||: 0.014676\n",
      "Loss: 0.014624; ||g||: 0.014467\n",
      "Loss: 0.014417; ||g||: 0.014263\n",
      "Loss: 0.014215; ||g||: 0.014066\n",
      "Loss: 0.014018; ||g||: 0.013873\n",
      "Loss: 0.013827; ||g||: 0.013686\n",
      "Loss: 0.013641; ||g||: 0.013504\n",
      "Loss: 0.013460; ||g||: 0.013326\n",
      "Loss: 0.013283; ||g||: 0.013153\n",
      "Loss: 0.013111; ||g||: 0.012984\n",
      "Loss: 0.012944; ||g||: 0.012820\n",
      "Loss: 0.012781; ||g||: 0.012660\n",
      "Loss: 0.012621; ||g||: 0.012504\n",
      "Loss: 0.012466; ||g||: 0.012351\n",
      "Loss: 0.012314; ||g||: 0.012202\n",
      "Loss: 0.012166; ||g||: 0.012057\n",
      "Loss: 0.012022; ||g||: 0.011915\n",
      "Loss: 0.011881; ||g||: 0.011776\n",
      "Loss: 0.011743; ||g||: 0.011641\n",
      "Loss: 0.011608; ||g||: 0.011508\n",
      "Loss: 0.011476; ||g||: 0.011379\n",
      "Loss: 0.011348; ||g||: 0.011252\n",
      "Loss: 0.011222; ||g||: 0.011128\n",
      "Loss: 0.011098; ||g||: 0.011007\n",
      "Loss: 0.010978; ||g||: 0.010889\n",
      "Loss: 0.010860; ||g||: 0.010773\n",
      "Loss: 0.010745; ||g||: 0.010659\n",
      "Loss: 0.010632; ||g||: 0.010548\n",
      "Loss: 0.010521; ||g||: 0.010439\n",
      "Loss: 0.010412; ||g||: 0.010332\n",
      "Loss: 0.010306; ||g||: 0.010228\n",
      "Loss: 0.010202; ||g||: 0.010125\n",
      "Loss: 0.010100; ||g||: 0.010025\n",
      "Loss: 0.010000; ||g||: 0.009926\n",
      "Loss: 0.009902; ||g||: 0.009829\n",
      "tensor([[4.2120],\n",
      "        [0.0104]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "X = torch.Tensor([[-1, 1, 10],[1, 1, 1]])\n",
    "y = torch.Tensor([-1, 1, 1])\n",
    "w = torch.Tensor([[0.1],[0.1]])\n",
    "alpha = 1\n",
    "print(X)\n",
    "print(y)\n",
    "print(w)\n",
    "for iter in range(100):\n",
    "    tmp = torch.exp(torch.matmul(torch.transpose(w,0,1),X)*(-y))\n",
    "\n",
    "    ##############################\n",
    "    ## Use tmp to compute f and g. Instead of summing we average the result, i.e.,\n",
    "    ## complete only inside torch.mean(...) and don't remove this function\n",
    "    ## Dimensions: f (scalar); g (2)\n",
    "    ##############################\n",
    "    f = torch.mean(torch.log(1+tmp))\n",
    "    g = torch.mean((-y)*tmp/(1+tmp)*X,1)\n",
    "\n",
    "    print(\"Loss: %f; ||g||: %f\" % (f, torch.norm(g)))\n",
    "    g = g.view(-1,1)\n",
    "    w = w - alpha*g\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.615214; ||g||: 0.613350\n",
      "Loss: 0.332295; ||g||: 0.332677\n",
      "Loss: 0.238526; ||g||: 0.237049\n",
      "Loss: 0.188502; ||g||: 0.187143\n",
      "Loss: 0.156530; ||g||: 0.155550\n",
      "Loss: 0.134098; ||g||: 0.133439\n",
      "Loss: 0.117414; ||g||: 0.116985\n",
      "Loss: 0.104489; ||g||: 0.104218\n",
      "Loss: 0.094169; ||g||: 0.094005\n",
      "Loss: 0.085731; ||g||: 0.085640\n",
      "Loss: 0.078700; ||g||: 0.078658\n",
      "Loss: 0.072749; ||g||: 0.072740\n",
      "Loss: 0.067645; ||g||: 0.067658\n",
      "Loss: 0.063218; ||g||: 0.063246\n",
      "Loss: 0.059342; ||g||: 0.059380\n",
      "Loss: 0.055918; ||g||: 0.055962\n",
      "Loss: 0.052872; ||g||: 0.052920\n",
      "Loss: 0.050145; ||g||: 0.050194\n",
      "Loss: 0.047687; ||g||: 0.047738\n",
      "Loss: 0.045462; ||g||: 0.045512\n",
      "Loss: 0.043437; ||g||: 0.043487\n",
      "Loss: 0.041586; ||g||: 0.041635\n",
      "Loss: 0.039889; ||g||: 0.039936\n",
      "Loss: 0.038325; ||g||: 0.038371\n",
      "Loss: 0.036881; ||g||: 0.036925\n",
      "Loss: 0.035542; ||g||: 0.035584\n",
      "Loss: 0.034298; ||g||: 0.034339\n",
      "Loss: 0.033139; ||g||: 0.033178\n",
      "Loss: 0.032056; ||g||: 0.032094\n",
      "Loss: 0.031043; ||g||: 0.031078\n",
      "Loss: 0.030092; ||g||: 0.030126\n",
      "Loss: 0.029198; ||g||: 0.029230\n",
      "Loss: 0.028356; ||g||: 0.028387\n",
      "Loss: 0.027561; ||g||: 0.027591\n",
      "Loss: 0.026810; ||g||: 0.026839\n",
      "Loss: 0.026100; ||g||: 0.026127\n",
      "Loss: 0.025426; ||g||: 0.025452\n",
      "Loss: 0.024786; ||g||: 0.024811\n",
      "Loss: 0.024178; ||g||: 0.024202\n",
      "Loss: 0.023600; ||g||: 0.023623\n",
      "Loss: 0.023048; ||g||: 0.023070\n",
      "Loss: 0.022522; ||g||: 0.022543\n",
      "Loss: 0.022020; ||g||: 0.022040\n",
      "Loss: 0.021539; ||g||: 0.021558\n",
      "Loss: 0.021079; ||g||: 0.021098\n",
      "Loss: 0.020639; ||g||: 0.020657\n",
      "Loss: 0.020217; ||g||: 0.020234\n",
      "Loss: 0.019811; ||g||: 0.019828\n",
      "Loss: 0.019422; ||g||: 0.019438\n",
      "Loss: 0.019048; ||g||: 0.019063\n",
      "Loss: 0.018688; ||g||: 0.018703\n",
      "Loss: 0.018341; ||g||: 0.018356\n",
      "Loss: 0.018008; ||g||: 0.018021\n",
      "Loss: 0.017686; ||g||: 0.017699\n",
      "Loss: 0.017375; ||g||: 0.017388\n",
      "Loss: 0.017075; ||g||: 0.017088\n",
      "Loss: 0.016786; ||g||: 0.016798\n",
      "Loss: 0.016506; ||g||: 0.016518\n",
      "Loss: 0.016236; ||g||: 0.016247\n",
      "Loss: 0.015974; ||g||: 0.015984\n",
      "Loss: 0.015720; ||g||: 0.015731\n",
      "Loss: 0.015475; ||g||: 0.015485\n",
      "Loss: 0.015237; ||g||: 0.015246\n",
      "Loss: 0.015006; ||g||: 0.015015\n",
      "Loss: 0.014782; ||g||: 0.014791\n",
      "Loss: 0.014565; ||g||: 0.014574\n",
      "Loss: 0.014354; ||g||: 0.014363\n",
      "Loss: 0.014150; ||g||: 0.014158\n",
      "Loss: 0.013950; ||g||: 0.013958\n",
      "Loss: 0.013757; ||g||: 0.013765\n",
      "Loss: 0.013569; ||g||: 0.013576\n",
      "Loss: 0.013386; ||g||: 0.013393\n",
      "Loss: 0.013208; ||g||: 0.013215\n",
      "Loss: 0.013034; ||g||: 0.013041\n",
      "Loss: 0.012865; ||g||: 0.012872\n",
      "Loss: 0.012701; ||g||: 0.012707\n",
      "Loss: 0.012540; ||g||: 0.012546\n",
      "Loss: 0.012384; ||g||: 0.012390\n",
      "Loss: 0.012231; ||g||: 0.012237\n",
      "Loss: 0.012082; ||g||: 0.012088\n",
      "Loss: 0.011937; ||g||: 0.011943\n",
      "Loss: 0.011795; ||g||: 0.011801\n",
      "Loss: 0.011657; ||g||: 0.011662\n",
      "Loss: 0.011522; ||g||: 0.011527\n",
      "Loss: 0.011389; ||g||: 0.011395\n",
      "Loss: 0.011260; ||g||: 0.011265\n",
      "Loss: 0.011134; ||g||: 0.011139\n",
      "Loss: 0.011011; ||g||: 0.011016\n",
      "Loss: 0.010890; ||g||: 0.010895\n",
      "Loss: 0.010772; ||g||: 0.010777\n",
      "Loss: 0.010657; ||g||: 0.010661\n",
      "Loss: 0.010543; ||g||: 0.010548\n",
      "Loss: 0.010433; ||g||: 0.010437\n",
      "Loss: 0.010324; ||g||: 0.010329\n",
      "Loss: 0.010218; ||g||: 0.010222\n",
      "Loss: 0.010114; ||g||: 0.010118\n",
      "Loss: 0.010013; ||g||: 0.010016\n",
      "Loss: 0.009913; ||g||: 0.009916\n",
      "Loss: 0.009815; ||g||: 0.009818\n",
      "Loss: 0.009719; ||g||: 0.009722\n",
      "tensor([[4.2385],\n",
      "        [0.0408]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "X = torch.Tensor([[-1, 1, 2],[1, 1, 1]])\n",
    "y = torch.Tensor([-1, 1, 1])\n",
    "w = torch.Tensor([[0.1],[0.1]])\n",
    "w.requires_grad = True\n",
    "alpha = 1\n",
    "#lr: learning rate\n",
    "optimizer = optim.SGD([w], lr=alpha)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for iter in range(100):\n",
    "    tmp = torch.exp(torch.matmul(torch.transpose(w,0,1),X)*(-y))\n",
    "\n",
    "    ##############################\n",
    "    ## loss is the same as f in A2_LogisticRegression.py\n",
    "    ## Dimensions: loss (scalar)\n",
    "    ##############################\n",
    "    loss = torch.mean(torch.log(1+tmp))\n",
    "\n",
    "    loss.backward()\n",
    "    print(\"Loss: %f; ||g||: %f\" % (loss, torch.norm(w.grad)))\n",
    "\n",
    "    ##############################\n",
    "    ## Use two functions within the optimizer instance to perform the update step\n",
    "    ##############################\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "X = torch.Tensor([[-1, 1, 2],[1, 1, 1]])\n",
    "y = torch.Tensor([-1,1,1])\n",
    "w = torch.Tensor([[0.1],[0.1]])\n",
    "w.requires_grad = True\n",
    "alpha = 1\n",
    "#lr: learning rate\n",
    "optimizer = optim.SGD([w], lr=alpha)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "tmp = torch.exp(torch.matmul(torch.transpose(w,0,1),X)*(-y))\n",
    "\n",
    "    ##############################\n",
    "    ## loss is the same as f in A2_LogisticRegression.py\n",
    "    ## Dimensions: loss (scalar)\n",
    "    ##############################\n",
    "loss = torch.mean(torch.log(1+tmp))\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3).random_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShallowNet(\n",
      "  (fc1): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "X = torch.Tensor([[-1, 1, 2],[1, 1, 1]])\n",
    "##############################\n",
    "## modify the dataset so that it can be used here and is equivalent to the one \n",
    "## used in A2_LogisticRegression2.py and A2_LogisticRegression.py\n",
    "## Dimensions: y (3)\n",
    "##############################\n",
    "y = torch.Tensor([0,1,1])\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "class ShallowNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1, bias=False)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.fc1(X)\n",
    "\n",
    "net = ShallowNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.2000, 0.3000], grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "net.fc1.weight.data = torch.Tensor([[0.1, 0.1]])\n",
    "\n",
    "print(net(torch.transpose(X,0,1)).squeeze())\n",
    "print(net(torch.transpose(X,0,1)).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.615214; ||g||: 0.613350\n",
      "Loss: 0.332295; ||g||: 0.332677\n",
      "Loss: 0.238526; ||g||: 0.237049\n",
      "Loss: 0.188502; ||g||: 0.187143\n",
      "Loss: 0.156530; ||g||: 0.155550\n",
      "Loss: 0.134098; ||g||: 0.133439\n",
      "Loss: 0.117414; ||g||: 0.116985\n",
      "Loss: 0.104489; ||g||: 0.104218\n",
      "Loss: 0.094169; ||g||: 0.094005\n",
      "Loss: 0.085731; ||g||: 0.085640\n",
      "Loss: 0.078700; ||g||: 0.078658\n",
      "Loss: 0.072749; ||g||: 0.072740\n",
      "Loss: 0.067645; ||g||: 0.067658\n",
      "Loss: 0.063218; ||g||: 0.063246\n",
      "Loss: 0.059342; ||g||: 0.059380\n",
      "Loss: 0.055918; ||g||: 0.055962\n",
      "Loss: 0.052872; ||g||: 0.052920\n",
      "Loss: 0.050144; ||g||: 0.050194\n",
      "Loss: 0.047687; ||g||: 0.047738\n",
      "Loss: 0.045462; ||g||: 0.045512\n",
      "Loss: 0.043437; ||g||: 0.043487\n",
      "Loss: 0.041586; ||g||: 0.041635\n",
      "Loss: 0.039889; ||g||: 0.039936\n",
      "Loss: 0.038325; ||g||: 0.038371\n",
      "Loss: 0.036881; ||g||: 0.036925\n",
      "Loss: 0.035542; ||g||: 0.035584\n",
      "Loss: 0.034298; ||g||: 0.034339\n",
      "Loss: 0.033139; ||g||: 0.033178\n",
      "Loss: 0.032056; ||g||: 0.032094\n",
      "Loss: 0.031043; ||g||: 0.031078\n",
      "Loss: 0.030092; ||g||: 0.030126\n",
      "Loss: 0.029198; ||g||: 0.029230\n",
      "Loss: 0.028356; ||g||: 0.028387\n",
      "Loss: 0.027561; ||g||: 0.027591\n",
      "Loss: 0.026810; ||g||: 0.026839\n",
      "Loss: 0.026100; ||g||: 0.026127\n",
      "Loss: 0.025426; ||g||: 0.025452\n",
      "Loss: 0.024786; ||g||: 0.024811\n",
      "Loss: 0.024178; ||g||: 0.024202\n",
      "Loss: 0.023600; ||g||: 0.023622\n",
      "Loss: 0.023048; ||g||: 0.023070\n",
      "Loss: 0.022522; ||g||: 0.022543\n",
      "Loss: 0.022020; ||g||: 0.022040\n",
      "Loss: 0.021539; ||g||: 0.021558\n",
      "Loss: 0.021079; ||g||: 0.021098\n",
      "Loss: 0.020639; ||g||: 0.020657\n",
      "Loss: 0.020217; ||g||: 0.020234\n",
      "Loss: 0.019811; ||g||: 0.019828\n",
      "Loss: 0.019422; ||g||: 0.019438\n",
      "Loss: 0.019048; ||g||: 0.019063\n",
      "Loss: 0.018688; ||g||: 0.018703\n",
      "Loss: 0.018341; ||g||: 0.018356\n",
      "Loss: 0.018008; ||g||: 0.018021\n",
      "Loss: 0.017686; ||g||: 0.017699\n",
      "Loss: 0.017375; ||g||: 0.017388\n",
      "Loss: 0.017075; ||g||: 0.017088\n",
      "Loss: 0.016786; ||g||: 0.016798\n",
      "Loss: 0.016506; ||g||: 0.016518\n",
      "Loss: 0.016236; ||g||: 0.016247\n",
      "Loss: 0.015974; ||g||: 0.015984\n",
      "Loss: 0.015720; ||g||: 0.015731\n",
      "Loss: 0.015475; ||g||: 0.015485\n",
      "Loss: 0.015237; ||g||: 0.015246\n",
      "Loss: 0.015006; ||g||: 0.015015\n",
      "Loss: 0.014782; ||g||: 0.014791\n",
      "Loss: 0.014565; ||g||: 0.014574\n",
      "Loss: 0.014354; ||g||: 0.014363\n",
      "Loss: 0.014150; ||g||: 0.014158\n",
      "Loss: 0.013950; ||g||: 0.013958\n",
      "Loss: 0.013757; ||g||: 0.013765\n",
      "Loss: 0.013569; ||g||: 0.013576\n",
      "Loss: 0.013386; ||g||: 0.013393\n",
      "Loss: 0.013208; ||g||: 0.013215\n",
      "Loss: 0.013034; ||g||: 0.013041\n",
      "Loss: 0.012865; ||g||: 0.012872\n",
      "Loss: 0.012701; ||g||: 0.012707\n",
      "Loss: 0.012540; ||g||: 0.012546\n",
      "Loss: 0.012384; ||g||: 0.012390\n",
      "Loss: 0.012231; ||g||: 0.012237\n",
      "Loss: 0.012082; ||g||: 0.012088\n",
      "Loss: 0.011937; ||g||: 0.011943\n",
      "Loss: 0.011795; ||g||: 0.011801\n",
      "Loss: 0.011657; ||g||: 0.011662\n",
      "Loss: 0.011522; ||g||: 0.011527\n",
      "Loss: 0.011389; ||g||: 0.011395\n",
      "Loss: 0.011260; ||g||: 0.011265\n",
      "Loss: 0.011134; ||g||: 0.011139\n",
      "Loss: 0.011011; ||g||: 0.011015\n",
      "Loss: 0.010890; ||g||: 0.010895\n",
      "Loss: 0.010772; ||g||: 0.010777\n",
      "Loss: 0.010657; ||g||: 0.010661\n",
      "Loss: 0.010543; ||g||: 0.010548\n",
      "Loss: 0.010433; ||g||: 0.010437\n",
      "Loss: 0.010324; ||g||: 0.010329\n",
      "Loss: 0.010218; ||g||: 0.010222\n",
      "Loss: 0.010114; ||g||: 0.010118\n",
      "Loss: 0.010013; ||g||: 0.010016\n",
      "Loss: 0.009913; ||g||: 0.009916\n",
      "Loss: 0.009815; ||g||: 0.009818\n",
      "Loss: 0.009719; ||g||: 0.009722\n",
      "Parameter containing:\n",
      "tensor([[4.2385, 0.0408]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=alpha)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for iter in range(100):\n",
    "    netOutput = net(torch.transpose(X,0,1)).squeeze()\n",
    "\n",
    "    ##############################\n",
    "    ## provide the arguments for the criterion function\n",
    "    ##############################\n",
    "    loss = criterion(netOutput, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    gn = 0\n",
    "    for f in net.parameters():\n",
    "        gn = gn + torch.norm(f.grad)\n",
    "    print(\"Loss: %f; ||g||: %f\" % (loss, gn))\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "for f in net.parameters():\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [4., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 4.5000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([[1.,2.],[4.,5.]])\n",
    "print(a)\n",
    "torch.mean(a,1\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  1., 10.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =torch.tensor([1.,2.,3])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  2., 30.],\n",
       "        [ 1.,  2.,  3.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  2., 30.],\n",
       "        [ 1.,  2.,  3.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
